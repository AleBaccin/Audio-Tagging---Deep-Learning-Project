{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import audio_tagging_utils as utils\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import tensorflow.keras.backend as K\n",
    "import gc\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# This is needed to get if the gpu is detected, as we carried out runs on our local machines.\n",
    "# print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSjOU2eBbUyR"
   },
   "source": [
    "A CNN within our project will be fed the spectrograms of the .wav as input images. Such images need to be generated first, this is carried out by calling a method defined in the utility file. Such method will fetch each .wav present in the input directory and traspose it to its corresponding spectrogram, saving it as a .jpg image. The project relied a lot on os.path.join as to make paths os agnostic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf_dir = os.path.join('meta', 'train.csv')\n",
    "testdf_dir = os.path.join('meta', 'test.csv')\n",
    "train_path = os.path.join('images', 'train')\n",
    "test_path = os.path.join('images', 'test')\n",
    "number_of_splits = 5\n",
    "number_of_classes = 41\n",
    "\n",
    "if not os.path.exists('runs'):\n",
    "    os.mkdir('runs')\n",
    "\n",
    "if not os.path.exists('images'):\n",
    "    os.mkdir('images')\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    os.mkdir(train_path)\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    os.mkdir(test_path)\n",
    "\n",
    "if not len(glob(os.path.join(train_path, '*'))) == 9473:\n",
    "    utils.create_images('train', 'train')\n",
    "\n",
    "if not len(glob(os.path.join(test_path, '*'))) == 1600:\n",
    "    utils.create_images('test', 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbMBfAgQJNTD"
   },
   "source": [
    "As the code aim to be highly modular, each employed model is defined as a method, this makes the Notebook more organic and readable, as each model is encapsulated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers, optimizers\n",
    "from tensorflow.python.keras.engine import training\n",
    "from tensorflow.python.framework.ops import Tensor\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, LeakyReLU, Conv2D, MaxPooling2D, LSTM, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def spectrogram_2d_conv_pool_cnn(model_input: Tensor) -> training.Model:\n",
    "    x = Conv2D(32, (3, 3), padding='same', activation='relu')(model_input)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(number_of_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(model_input, x, name='spectrogram_2d_conv_pool_cnn')\n",
    "    model.compile(optimizers.Adam(0.001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def kaggle_2d_conv_pool_cnn(model_input: Tensor) -> training.Model:\n",
    "    x = Conv2D(32, (4,10), padding=\"same\")(model_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "    \n",
    "    x = Conv2D(32, (4,10), padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    x = Dense(number_of_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(model_input, x, name='kaggle_2d_conv_pool_cnn')\n",
    "    model.compile(optimizers.Adam(0.0001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def kaggle_1d_conv_pool_cnn(model_input: Tensor) -> training.Model:\n",
    "    x = Conv1D(16, 9, activation='relu', padding=\"valid\")(model_input)\n",
    "    x = Conv1D(16, 9, activation='relu', padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(16)(x)\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    \n",
    "    x = Conv1D(32, 3, activation='relu', padding=\"valid\")(x)\n",
    "    x = Conv1D(32, 3, activation='relu', padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    \n",
    "    x = Conv1D(32, 3, activation='relu', padding=\"valid\")(x)\n",
    "    x = Conv1D(32, 3, activation='relu', padding=\"valid\")(x)\n",
    "    x = MaxPooling1D(4)(x)\n",
    "    x = Dropout(rate=0.1)(x)\n",
    "    \n",
    "    x = Conv1D(256, 3, activation='relu', padding=\"valid\")(x)\n",
    "    x = Conv1D(256, 3, activation='relu', padding=\"valid\")(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    x = Dropout(rate=0.2)(x)\n",
    "\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(number_of_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(model_input, x, name='kaggle_1d_conv_pool_cnn')\n",
    "    model.compile(optimizers.Adam(0.001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def lstm(model_input: Tensor) -> training.Model:\n",
    "    x = LSTM(1028, activation='relu')(model_input)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(number_of_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(model_input, x, name='lstm')\n",
    "    model.compile(optimizers.Adam(0.001),loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to set up our trainining and evaluation pipeline for the model employing the .jpg spectrograms. The pipeline implement Kfold validation during training and evaluate each model on the test set, as well as generating its predictions using the best model obtained, as the best weights are saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the input to the CNN, this shape is obtained from the flow_from_dataset method implemented in the next cell.\n",
    "model_input = Input(shape=(64, 64, 3))\n",
    "spectrogram_2d_model = spectrogram_2d_conv_pool_cnn(model_input)\n",
    "spectrogram_2d_model.summary()\n",
    "\n",
    "# We generate a folder for each model, this allows for a clear separation of the runs. Allowing for a cleaner folder structure.\n",
    "# The other folders are generated in the cells below\n",
    "run_dir = os.path.join('runs', spectrogram_2d_model.name)\n",
    "if not os.path.exists(run_dir):\n",
    "    os.mkdir(run_dir)\n",
    "if os.path.exists(os.path.join(run_dir, 'logs')):\n",
    "    shutil.rmtree(os.path.join(run_dir, 'logs'))\n",
    "    \n",
    "traindf=pd.read_csv(traindf_dir)\n",
    "testdf=pd.read_csv(testdf_dir)    \n",
    "\n",
    "# As the datasets have .wav, they need to be modified to search for .jpg files\n",
    "traindf[\"fname\"]= traindf[\"fname\"].apply(utils.append_ext)\n",
    "testdf[\"fname\"]= testdf[\"fname\"].apply(utils.append_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "# The class indices to encode the labels need to be static(if not, ensemble of predictiosn will not be correct)\n",
    "class_indices = {}\n",
    "\n",
    "kfold_validation = KFold(n_splits= number_of_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# This structure is based on the one followed by the Kaggle notebook: https://www.kaggle.com/fizzbuzz/beginner-s-guide-to-audio-data\n",
    "#Using random_state = 42 for repeatibility\n",
    "print(f'+-----------Training {spectrogram_2d_model.name} Model-----------+')\n",
    "for i, (train_split_indexes, test_split_indexes) in enumerate(kfold_validation.split(traindf)):\n",
    "    train_fold = traindf.iloc[train_split_indexes]\n",
    "    val_fold = traindf.iloc[test_split_indexes]\n",
    "\n",
    "    best_weights_file = os.path.join(run_dir, f'best_{i}.h5')\n",
    "    checkpoint = ModelCheckpoint(best_weights_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    tb = TensorBoard(log_dir= os.path.join(run_dir, 'logs', f'fold_{i}'), write_graph=True)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tb]\n",
    "\n",
    "    # flow_from_dataframe is useful as it generates the bitstreams to pass to the model give the image folder and reference dataframe\n",
    "    train_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=train_fold,\n",
    "        directory=os.path.join('images', 'train'),\n",
    "        x_col=\"fname\",\n",
    "        y_col=\"label\",\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(64,64))\n",
    "\n",
    "    valid_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=val_fold,\n",
    "        directory=os.path.join('images', 'train'),\n",
    "        x_col=\"fname\",\n",
    "        y_col=\"label\",\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "        shuffle=True,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(64,64))\n",
    "\n",
    "    STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "    STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "    spectrogram_2d_model.fit(train_generator,\n",
    "                    callbacks=callbacks_list,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=25)\n",
    "\n",
    "    spectrogram_2d_model.load_weights(best_weights_file)\n",
    "    \n",
    "    train_generator.reset()\n",
    "    valid_generator.reset()\n",
    "    \n",
    "    eval_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=testdf,\n",
    "        directory=os.path.join('images', 'test'),\n",
    "        x_col=\"fname\",\n",
    "        y_col= \"label\",\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        class_mode=\"categorical\",\n",
    "        target_size=(64,64))\n",
    "\n",
    "    STEP_SIZE_EVAL=eval_generator.n//eval_generator.batch_size\n",
    "\n",
    "    # It is important to reset the generator before evaluation\n",
    "    eval_generator.reset()\n",
    "    \n",
    "    spectrogram_2d_model.evaluate(eval_generator, steps=STEP_SIZE_EVAL, verbose= 1)\n",
    "    \n",
    "    test_generator=datagen.flow_from_dataframe(\n",
    "        dataframe=testdf,\n",
    "        directory=os.path.join('images', 'test'),\n",
    "        x_col=\"fname\",\n",
    "        y_col= None,\n",
    "        batch_size=32,\n",
    "        seed=42,\n",
    "        shuffle=False,\n",
    "        class_mode= None,\n",
    "        target_size=(64,64))\n",
    "    \n",
    "    STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "    \n",
    "    test_generator.reset()\n",
    "    \n",
    "    pred = spectrogram_2d_model.predict(test_generator, steps=STEP_SIZE_TEST, verbose= 1)\n",
    "    \n",
    "    np.save(os.path.join(run_dir, f'test_predictions_{i}.npy'), pred)\n",
    "    \n",
    "    pd.DataFrame(spectrogram_2d_model.history.history).plot()\n",
    "    \n",
    "    #On last step, retrieve actual class_indices, this is used to retrieve the actual string labels\n",
    "    if i == number_of_splits - 1:\n",
    "        class_indices = train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensembling of the predictions using geometric average\n",
    "pred_list = []\n",
    "for i in range(number_of_splits):\n",
    "    pred_list.append(np.load(os.path.join(run_dir, f'test_predictions_{i}.npy')))\n",
    "\n",
    "prediction = np.ones_like(pred_list[0])\n",
    "\n",
    "for pred in pred_list:\n",
    "    prediction = prediction*pred\n",
    "prediction = prediction**(1./len(pred_list))\n",
    "\n",
    "#Saving predictions\n",
    "predicted_class_indices = np.argmax(prediction,axis=-1)\n",
    "\n",
    "#Classes need to be retrieved using the class_indices used by the train generators, for simplicity, the class indices will be used to the other two models.\n",
    "labels = dict((v,k) for k,v in class_indices.items())\n",
    "predicted_labels = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "test = pd.read_csv(testdf_dir)\n",
    "\n",
    "test[['fname', 'label']].to_csv(os.path.join(run_dir, f'{spectrogram_2d_model.name}_predictions.csv'), index=False)\n",
    "\n",
    "y_true = test['label']\n",
    "y_pred = predicted_labels\n",
    "\n",
    "print(f'+-----------Printing {spectrogram_2d_model.name} predictions evaluations-----------+')\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8cFBm1fJFe5"
   },
   "source": [
    "Now we will train two different models. One uses the Mfcc transformation (kaggle_2d) and the other uses the raw .wav files(kaggle_1d)\n",
    "The two models uses different sampling rate to generate the encodings. This is necessary due to mfcc requiring a higher sampling rate to produce good enough results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 2\n",
    "n_mfcc = 40\n",
    "\n",
    "#raw .wav and mfcc are obtained using different sample rates, we followed the values used on the Kaggle notebook previously cited.\n",
    "kaggle_2d_sr = 44100\n",
    "base_sr = 16000\n",
    "\n",
    "lstm_seed = 3\n",
    "kaggle_2d_seed = 2\n",
    "kaggle_1d_seed = 1\n",
    "\n",
    "#Model declaration\n",
    "model_input = Input(shape=utils.mfcc_input_sizes(n_mfcc, kaggle_2d_sr, max_len))\n",
    "kaggle_2d_model = kaggle_2d_conv_pool_cnn(model_input) \n",
    "\n",
    "\n",
    "model_input = Input(shape=utils.wav_input_sizes(base_sr, max_len))\n",
    "kaggle_1d_model = kaggle_1d_conv_pool_cnn(model_input) \n",
    "\n",
    "model_input = Input(shape=utils.mfcc_compressed_input_sizes(n_mfcc, max_len))\n",
    "lstm_model = lstm(model_input) \n",
    "\n",
    "kaggle_1d_run_dir = os.path.join('runs', kaggle_1d_model.name)\n",
    "\n",
    "if not os.path.exists(kaggle_1d_run_dir):\n",
    "    os.mkdir(kaggle_1d_run_dir)\n",
    "if os.path.exists(os.path.join(kaggle_1d_run_dir, 'logs')):\n",
    "    shutil.rmtree(os.path.join(kaggle_1d_run_dir, 'logs'))\n",
    "    \n",
    "kaggle_2d_run_dir = os.path.join('runs', kaggle_2d_model.name)\n",
    "\n",
    "if not os.path.exists(kaggle_2d_run_dir):\n",
    "    os.mkdir(kaggle_2d_run_dir)\n",
    "if os.path.exists(os.path.join(kaggle_2d_run_dir, 'logs')):\n",
    "    shutil.rmtree(os.path.join(kaggle_2d_run_dir, 'logs'))\n",
    "\n",
    "lstm_run_dir = os.path.join('runs', lstm_model.name)\n",
    "\n",
    "if not os.path.exists(lstm_run_dir):\n",
    "    os.mkdir(lstm_run_dir)\n",
    "if os.path.exists(os.path.join(lstm_run_dir, 'logs')):\n",
    "    shutil.rmtree(os.path.join(lstm_run_dir, 'logs'))\n",
    "    \n",
    "#Feeding the models into a list will make the code more modular.\n",
    "models_to_train = [kaggle_2d_model, kaggle_1d_model, lstm_model]\n",
    "\n",
    "#Re-read the dataframes, as .jpg was appended to 'fname'\n",
    "traindf=pd.read_csv(traindf_dir)\n",
    "testdf=pd.read_csv(testdf_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_validation = KFold(n_splits= number_of_splits, shuffle=True, random_state=kaggle_2d_seed)\n",
    "# Although this makes the code looks ugly, it allows for better testability and repetibility\n",
    "\n",
    "kaggle_2d_model.summary()\n",
    "\n",
    "run_dir = os.path.join('runs', kaggle_2d_model.name)\n",
    "test_data = os.path.join(run_dir, 'test_input_labels.npz')\n",
    "train_data = os.path.join(run_dir, 'train_input_labels.npz')\n",
    "\n",
    "if not os.path.exists(test_data) :\n",
    "    X_test, y_test = utils.create_mfcc_array(testdf, 'test', sr= kaggle_2d_sr, max_len= max_len, n_mfcc= n_mfcc)\n",
    "    np.savez(test_data, x_test=X_test, y_test=y_test)\n",
    "if os.path.exists(test_data):\n",
    "    test_arrays = np.load(test_data)\n",
    "    \n",
    "if not os.path.exists(train_data) :\n",
    "    X, y = utils.create_mfcc_array(traindf, 'train', sr= kaggle_2d_sr, max_len= max_len, n_mfcc= n_mfcc)\n",
    "    np.savez(train_data, x=X, y=y)\n",
    "if os.path.exists(train_data):\n",
    "    train_arrays = np.load(train_data)\n",
    "    \n",
    "#The models uses different input shapes\n",
    "input_shape = utils.mfcc_input_sizes(n_mfcc, kaggle_2d_sr, max_len)\n",
    "\n",
    "#Using random_state = 0 for repeatibility\n",
    "print(f'+-----------Training {kaggle_2d_model.name} Model-----------+')\n",
    "for i, (train_split_indexes, validation_split_indexes) in enumerate(kfold_validation.split(traindf)):\n",
    "    #Loading all necessary arrays\n",
    "    X, y = train_arrays['x'], train_arrays['y']\n",
    "    X_test, y_test = test_arrays['x_test'], test_arrays['y_test']\n",
    "    X_train, y_train = X[train_split_indexes], y[train_split_indexes]\n",
    "    X_val, y_val = X[validation_split_indexes], y[validation_split_indexes]\n",
    "    \n",
    "    best_weights_file = os.path.join(run_dir, f'best_{i}.h5')\n",
    "    checkpoint = ModelCheckpoint(best_weights_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    tb = TensorBoard(log_dir= os.path.join(run_dir, 'logs', f'fold_{i}'), write_graph=True)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tb]\n",
    "\n",
    "    #Outputs are generated using consistend indices\n",
    "    y_train, y_val = pd.Series(y_train).apply(lambda x: class_indices[x]), pd.Series(y_val).apply(lambda x: class_indices[x])\n",
    "    X_train, X_val = X_train.reshape(X_train.shape[0], input_shape[0], input_shape[1], input_shape[2]), X_val.reshape(X_val.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "    y_test = pd.Series(y_test).apply(lambda x: class_indices[x])\n",
    "    X_test = X_test.reshape(X_test.shape[0], input_shape[0], input_shape[1], input_shape[2])\n",
    "    \n",
    "    #Very important data normalization.\n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    #Normalizing sets using the X_train's value, it is important to use the X_train normalization values\n",
    "    X_train, X_val, X_test = (X_train - mean)/std, (X_val - mean)/std, (X_test - mean)/std\n",
    "    \n",
    "    # Creating class matrix\n",
    "    y_train_hot = to_categorical(y_train)\n",
    "    y_val_hot = to_categorical(y_val)\n",
    "    y_test_hot = to_categorical(y_test)\n",
    "    \n",
    "    kaggle_2d_model.fit(X_train, y_train_hot,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_data=(X_val, y_val_hot),\n",
    "                epochs=25)\n",
    "    \n",
    "    kaggle_2d_model.load_weights(best_weights_file)\n",
    "    \n",
    "    kaggle_2d_model.evaluate(X_test, y_test_hot)\n",
    "    \n",
    "    pred = kaggle_2d_model.predict(X_test, verbose= 1)\n",
    "\n",
    "    np.save(os.path.join(run_dir, f'test_predictions_{i}.npy'), pred)\n",
    "    \n",
    "    pd.DataFrame(kaggle_2d_model.history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_validation = KFold(n_splits= number_of_splits, shuffle=True, random_state=kaggle_1d_seed)\n",
    "\n",
    "kaggle_1d_model.summary()\n",
    "\n",
    "run_dir = os.path.join('runs', kaggle_1d_model.name)\n",
    "test_data = os.path.join(run_dir, 'test_input_labels.npz')\n",
    "train_data = os.path.join(run_dir, 'train_input_labels.npz')\n",
    "\n",
    "if not os.path.exists(test_data) :\n",
    "    X_test, y_test = utils.create_wav_array(testdf, 'test', sr= base_sr, max_len= max_len)\n",
    "    np.savez(test_data, x_test=X_test, y_test=y_test)\n",
    "if os.path.exists(test_data):\n",
    "    test_arrays = np.load(test_data)\n",
    "    \n",
    "if not os.path.exists(train_data) :\n",
    "    X, y = utils.create_wav_array(traindf, 'train', sr= base_sr, max_len= max_len)\n",
    "    np.savez(train_data, x=X, y=y)\n",
    "if os.path.exists(train_data):\n",
    "    train_arrays = np.load(train_data)\n",
    "    \n",
    "input_shape = utils.wav_input_sizes(base_sr, max_len)\n",
    "\n",
    "print(f'+-----------Training {kaggle_1d_model.name} Model-----------+')\n",
    "for i, (train_split_indexes, validation_split_indexes) in enumerate(kfold_validation.split(traindf)):\n",
    "    X, y = train_arrays['x'], train_arrays['y']\n",
    "    X_test, y_test = test_arrays['x_test'], test_arrays['y_test']\n",
    "    X_train, y_train = X[train_split_indexes], y[train_split_indexes]\n",
    "    X_val, y_val = X[validation_split_indexes], y[validation_split_indexes]\n",
    "    \n",
    "    best_weights_file = os.path.join(run_dir, f'best_{i}.h5')\n",
    "    checkpoint = ModelCheckpoint(best_weights_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    tb = TensorBoard(log_dir= os.path.join(run_dir, 'logs', f'fold_{i}'), write_graph=True)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tb]\n",
    "\n",
    "    y_train, y_val = pd.Series(y_train).apply(lambda x: class_indices[x]), pd.Series(y_val).apply(lambda x: class_indices[x])\n",
    "    X_train, X_val = X_train.reshape(X_train.shape[0], input_shape[0], input_shape[1]), X_val.reshape(X_val.shape[0], input_shape[0], input_shape[1])\n",
    "    y_test = pd.Series(y_test).apply(lambda x: class_indices[x])\n",
    "    X_test= X_test.reshape(X_test.shape[0], input_shape[0], input_shape[1])\n",
    "    \n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    X_train, X_val, X_test = (X_train - mean)/std, (X_val - mean)/std, (X_test - mean)/std\n",
    "    \n",
    "    y_train_hot = to_categorical(y_train)\n",
    "    y_val_hot = to_categorical(y_val)\n",
    "    y_test_hot = to_categorical(y_test)\n",
    "    \n",
    "    kaggle_1d_model.fit(X_train, y_train_hot,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_data=(X_val, y_val_hot),\n",
    "                epochs=1)\n",
    "    \n",
    "    kaggle_1d_model.load_weights(best_weights_file)\n",
    "    \n",
    "    kaggle_1d_model.evaluate(X_test, y_test_hot)\n",
    "    \n",
    "    pred = kaggle_1d_model.predict(X_test, verbose= 1)\n",
    "\n",
    "    np.save(os.path.join(run_dir, f'test_predictions_{i}.npy'), pred)\n",
    "    \n",
    "    pd.DataFrame(kaggle_1d_model.history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_validation = KFold(n_splits= number_of_splits, shuffle=True, random_state=lstm_seed)\n",
    "\n",
    "lstm_model.summary()\n",
    "\n",
    "run_dir = os.path.join('runs', lstm_model.name)\n",
    "test_data = os.path.join(run_dir, 'test_input_labels.npz')\n",
    "train_data = os.path.join(run_dir, 'train_input_labels.npz')\n",
    "\n",
    "if not os.path.exists(test_data) :\n",
    "    X_test, y_test = utils.create_mfcc_array_compressed(testdf, 'test', sr= base_sr, max_len= max_len, n_mfcc=n_mfcc)\n",
    "    np.savez(test_data, x_test=X_test, y_test=y_test)\n",
    "if os.path.exists(test_data):\n",
    "    test_arrays = np.load(test_data)\n",
    "    \n",
    "if not os.path.exists(train_data) :\n",
    "    X, y = utils.create_mfcc_array_compressed(traindf, 'train', sr= base_sr, max_len= max_len, n_mfcc=n_mfcc)\n",
    "    np.savez(train_data, x=X, y=y)\n",
    "if os.path.exists(train_data):\n",
    "    train_arrays = np.load(train_data)\n",
    "    \n",
    "input_shape = utils.mfcc_compressed_input_sizes(n_mfcc, max_len)\n",
    "\n",
    "print(f'+-----------Training {lstm_model.name} Model-----------+')\n",
    "for i, (train_split_indexes, validation_split_indexes) in enumerate(kfold_validation.split(traindf)):\n",
    "    X, y = train_arrays['x'], train_arrays['y']\n",
    "    X_test, y_test = test_arrays['x_test'], test_arrays['y_test']\n",
    "    X_train, y_train = X[train_split_indexes], y[train_split_indexes]\n",
    "    X_val, y_val = X[validation_split_indexes], y[validation_split_indexes]\n",
    "    \n",
    "    best_weights_file = os.path.join(run_dir, f'best_{i}.h5')\n",
    "    checkpoint = ModelCheckpoint(best_weights_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n",
    "    tb = TensorBoard(log_dir= os.path.join(run_dir, 'logs', f'fold_{i}'), write_graph=True)\n",
    "\n",
    "    callbacks_list = [checkpoint, early, tb]\n",
    "\n",
    "    y_train, y_val = pd.Series(y_train).apply(lambda x: class_indices[x]), pd.Series(y_val).apply(lambda x: class_indices[x])\n",
    "    X_train, X_val = X_train.reshape(X_train.shape[0], input_shape[0], input_shape[1]), X_val.reshape(X_val.shape[0], input_shape[0], input_shape[1])\n",
    "    y_test = pd.Series(y_test).apply(lambda x: class_indices[x])\n",
    "    X_test= X_test.reshape(X_test.shape[0], input_shape[0], input_shape[1])\n",
    "    \n",
    "    mean = np.mean(X_train, axis=0)\n",
    "    std = np.std(X_train, axis=0)\n",
    "    \n",
    "    X_train, X_val, X_test = (X_train - mean)/std, (X_val - mean)/std, (X_test - mean)/std\n",
    "    \n",
    "    y_train_hot = to_categorical(y_train)\n",
    "    y_val_hot = to_categorical(y_val)\n",
    "    y_test_hot = to_categorical(y_test)\n",
    "    \n",
    "    lstm_model.fit(X_train, y_train_hot,\n",
    "                callbacks=callbacks_list,\n",
    "                validation_data=(X_val, y_val_hot),\n",
    "                epochs=50)\n",
    "    \n",
    "    lstm_model.load_weights(best_weights_file)\n",
    "    \n",
    "    lstm_model.evaluate(X_test, y_test_hot)\n",
    "    \n",
    "    pred = lstm_model.predict(X_test, verbose= 1)\n",
    "\n",
    "    np.save(os.path.join(run_dir, f'test_predictions_{i}.npy'), pred)\n",
    "    \n",
    "    pd.DataFrame(lstm_model.history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the obtained metrics for each model\n",
    "for model in models_to_train:\n",
    "    pred_list = []\n",
    "    for i in range(number_of_splits):\n",
    "        pred_list.append(np.load(os.path.join('runs', model.name, f'test_predictions_{i}.npy')))\n",
    "    \n",
    "    prediction = np.ones_like(pred_list[0])\n",
    "    \n",
    "    for pred in pred_list:\n",
    "        prediction = prediction*pred\n",
    "    prediction = prediction**(1./len(pred_list))\n",
    "   \n",
    "    # Generate predictions\n",
    "    predicted_class_indices = np.argmax(prediction,axis=-1)\n",
    "    \n",
    "    labels = dict((v,k) for k,v in class_indices.items())\n",
    "    predicted_labels = [labels[k] for k in predicted_class_indices]\n",
    "    \n",
    "    test = pd.read_csv(testdf_dir)\n",
    "    test[['fname', 'label']].to_csv(os.path.join('runs', model.name, f'{model.name}_predictions.csv'), index=False)\n",
    "    \n",
    "    y_true = test['label']\n",
    "    y_pred = predicted_labels\n",
    "    \n",
    "    print(f'+-----------Printing {model.name} predictions evaluation-----------+')\n",
    "    print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to ensemple all of our model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensembling all the models using Geometric mean.\n",
    "pred_list = []\n",
    "for i in range(number_of_splits):\n",
    "    pred_list.append(np.load(os.path.join('runs', spectrogram_2d_model.name, f'test_predictions_{i}.npy')))\n",
    "    \n",
    "for model in models_to_train:\n",
    "    for i in range(number_of_splits):\n",
    "        pred_list.append(np.load(os.path.join('runs', model.name, f'test_predictions_{i}.npy')))\n",
    "\n",
    "prediction = np.ones_like(pred_list[0])\n",
    "\n",
    "for pred in pred_list:\n",
    "    prediction = prediction*pred\n",
    "prediction = prediction**(1./len(pred_list))\n",
    "\n",
    "# Generate predictions\n",
    "\n",
    "predicted_class_indices = np.argmax(prediction,axis=-1)\n",
    "labels = (train_generator.class_indices)\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "predicted_labels = [labels[k] for k in predicted_class_indices]\n",
    "\n",
    "test = pd.read_csv(testdf_dir)\n",
    "test[['fname', 'label']].to_csv(f'ensembled_predictions.csv', index=False)\n",
    "\n",
    "y_true = test['label']\n",
    "y_pred = predicted_labels\n",
    "print(metrics.classification_report(y_true, y_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "audio_tagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('vvenv': conda)",
   "language": "python",
   "name": "python37764bitvvenvcondafa48b78b670947d8bc11c0248ea20194"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
